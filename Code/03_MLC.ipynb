{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#      MLC Script using Quadratic Discriminant Analysis for Ground-Cover Classification\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn tools\n",
    "from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score, f1_score\n",
    "import joblib  \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------- portable paths -------------------------\n",
    "REPO_ROOT   = Path(__file__).resolve().parent\n",
    "DATASET_DIR = Path(os.getenv(\"DATASET\", REPO_ROOT / \"Dataset\"))\n",
    "OUTPUT_DIR  = Path(os.getenv(\"MLC_OUTPUT\",  REPO_ROOT / \"Output\"))\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_PATH = DATASET_DIR / \"train\"\n",
    "VAL_PATH   = DATASET_DIR / \"val\"\n",
    "TEST_PATH  = DATASET_DIR / \"test\"\n",
    "\n",
    "# ------------------------Discover RGB images & their masks--------------------------------\n",
    "def list_rgb_and_masks(folder: str):\n",
    "    rgb = sorted([\n",
    "        os.path.join(folder, f)\n",
    "        for f in os.listdir(folder)\n",
    "        if f.lower().endswith(\".tif\") and not f.lower().endswith(\"_labelled.tif\")\n",
    "    ])\n",
    "    masks = sorted([\n",
    "        os.path.join(folder, f)\n",
    "        for f in os.listdir(folder)\n",
    "        if f.lower().endswith(\"_labelled.tif\")\n",
    "    ])\n",
    "    return rgb, masks\n",
    "\n",
    "train_image_files, train_mask_files = list_rgb_and_masks(TRAIN_PATH)\n",
    "val_image_files,   val_mask_files   = list_rgb_and_masks(VAL_PATH)\n",
    "test_image_files,  test_mask_files  = list_rgb_and_masks(TEST_PATH)\n",
    "\n",
    "# Optional: print a quick overview\n",
    "print(\"Collected filesâ†’\")\n",
    "for split, imgs, msks in [(\"Train\", train_image_files, train_mask_files),\n",
    "                          (\"Val\",   val_image_files,   val_mask_files),\n",
    "                          (\"Test\",  test_image_files,  test_mask_files)]:\n",
    "    print(f\"\\n{split} images ({len(imgs)}):\")\n",
    "    for p in imgs:\n",
    "        print(\"  \", os.path.basename(p))\n",
    "    print(f\"{split} masks   ({len(msks)}):\")\n",
    "    for p in msks:\n",
    "        print(\"  \", os.path.basename(p))\n",
    "\n",
    "n_classes = 8           # background = 0 ignored during patch extraction\n",
    "MAX_PATCHES_PER_IMAGE = 10000\n",
    "\n",
    "# -----------------------Utility Functions---------------------------------------\n",
    "def print_memory_usage():\n",
    "    mem = psutil.Process().memory_info().rss / 1024 ** 2\n",
    "    print(f\"Memory Usage: {mem:.2f} MB\")\n",
    "\n",
    "def get_label_file(image_file: str) -> str:\n",
    "    \"\"\"Return mask path matching *image_file* (expects suffix `_Labelled.tif`).\"\"\"\n",
    "    base = os.path.splitext(image_file)[0]\n",
    "    label_file = f\"{base}_Labelled.tif\"\n",
    "    if os.path.exists(label_file):\n",
    "        return label_file\n",
    "    raise FileNotFoundError(f\"No label file found for {image_file}\")\n",
    "\n",
    "\n",
    "def load_and_normalize_data(image_file):\n",
    "    try:\n",
    "        data = tifffile.imread(image_file)\n",
    "        if data.ndim == 2:\n",
    "            data = np.stack([data] * 3, axis=-1)\n",
    "        elif data.shape[-1] != 3:\n",
    "            raise ValueError(f\"Unexpected image shape: {data.shape}, expected 3 channels\")\n",
    "        \n",
    "        label_file = get_label_file(image_file)\n",
    "        labels = tifffile.imread(label_file)\n",
    "        \n",
    "        data_min, data_max = np.min(data), np.max(data)\n",
    "        data = (data - data_min) / (data_max - data_min + 1e-8)\n",
    "        data = data.astype(np.float32)\n",
    "        labels = labels.astype(np.int32)\n",
    "        return data, labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {image_file}: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "# -----------------------Patch Extraction---------------------------------------\n",
    "def extract_patches(image, label, kn, max_patches_per_image=None):\n",
    "    patch_size = 2 * kn + 1\n",
    "    padded_img = np.pad(image, ((kn, kn), (kn, kn), (0, 0)), mode='reflect')\n",
    "    \n",
    "    # 1) gather all valid coords\n",
    "    coords = [(i,j) for i in range(label.shape[0])\n",
    "                     for j in range(label.shape[1])\n",
    "                     if label[i,j] != 0]\n",
    "\n",
    "    # 2) if you want to limit to K patches, sample from coords\n",
    "    if max_patches_per_image and len(coords) > max_patches_per_image:\n",
    "        sampled_idxs = np.random.choice(len(coords),\n",
    "                                        max_patches_per_image,\n",
    "                                        replace=False)\n",
    "        coords = [coords[i] for i in sampled_idxs]\n",
    "\n",
    "    # 3) now build your patches & labels\n",
    "    patches, labels = [], []\n",
    "    for i,j in coords:\n",
    "        patch = padded_img[i:i+patch_size, j:j+patch_size, :]\n",
    "        patches.append(patch)\n",
    "        labels.append(label[i,j] - 1)   \n",
    "    return patches, labels\n",
    "\n",
    "# ---------------------Custom \"Dataset\" to Gather Patches----------------------------------\n",
    "class PatchDataset:\n",
    "    def __init__(self, image_files, kn=5, max_patches_per_image=None):\n",
    "        self.image_files = image_files\n",
    "        self.kn = kn\n",
    "        self.max_patches_per_image = max_patches_per_image\n",
    "\n",
    "        self.all_patches = []\n",
    "        self.all_labels = []\n",
    "        total_patches = 0\n",
    "\n",
    "        print(f\"\\n[Dataset Initialization] kn={kn}, max_patches_per_image={max_patches_per_image}\")\n",
    "        for idx, img_path in enumerate(self.image_files, start=1):\n",
    "            img_array, lbl_array = load_and_normalize_data(img_path)\n",
    "            if img_array is None or lbl_array is None:\n",
    "                print(f\"[WARNING] Skipping file due to error: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            patches, labels = extract_patches(img_array, lbl_array, self.kn, self.max_patches_per_image)\n",
    "            self.all_patches.extend(patches)\n",
    "            self.all_labels.extend(labels)\n",
    "            total_patches += len(patches)\n",
    "\n",
    "            print(f\"[INFO] Finished extracting patches for {idx}/{len(self.image_files)}: {img_path}\")\n",
    "        \n",
    "        print(f\"[INFO] Dataset created with total patches: {total_patches}\")\n",
    "        print_memory_usage()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.all_patches[idx], self.all_labels[idx]\n",
    "\n",
    "# -----------------------Build Datasets (Train, Val, Test)--------------------------------\n",
    "print(\"[INFO] Building Datasets...\")\n",
    "kn = 5\n",
    "start_build_time = time.time()\n",
    "\n",
    "train_dataset = PatchDataset(train_image_files, kn, MAX_PATCHES_PER_IMAGE)\n",
    "val_dataset   = PatchDataset(val_image_files,   kn, MAX_PATCHES_PER_IMAGE)\n",
    "test_dataset  = PatchDataset(test_image_files,  kn, MAX_PATCHES_PER_IMAGE)\n",
    "\n",
    "end_build_time = time.time()\n",
    "print(f\"\\n[INFO] Finished building all datasets in {end_build_time - start_build_time:.2f} seconds.\")\n",
    "\n",
    "def dataset_to_Xy(patch_dataset):\n",
    "    patches = patch_dataset.all_patches\n",
    "    labels  = patch_dataset.all_labels\n",
    "    X = np.array([p.reshape(-1) for p in patches], dtype=np.float32)\n",
    "    y = np.array(labels, dtype=np.int64)\n",
    "    return X, y\n",
    "\n",
    "print(\"[INFO] Converting training patches into (X_train, y_train)...\")\n",
    "X_train, y_train = dataset_to_Xy(train_dataset)\n",
    "print(f\" - Done. Shapes: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "\n",
    "print(\"[INFO] Converting validation patches into (X_val, y_val)...\")\n",
    "X_val, y_val = dataset_to_Xy(val_dataset)\n",
    "print(f\" - Done. Shapes: X_val={X_val.shape}, y_val={y_val.shape}\")\n",
    "\n",
    "print(\"[INFO] Converting test patches into (X_test, y_test)...\")\n",
    "X_test, y_test = dataset_to_Xy(test_dataset)\n",
    "print(f\" - Done. Shapes: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "# ----------------Enhanced Hyperparameter Tuning-----------------------------\n",
    "print(\"\\n[INFO] Starting Hyperparameter Search using RandomizedSearchCV...\")\n",
    "\n",
    "# Combine training and validation data for tuning \n",
    "X_combined = np.concatenate([X_train, X_val], axis=0)\n",
    "y_combined = np.concatenate([y_train, y_val], axis=0)\n",
    "\n",
    "# Create predefined split: -1 for training, 0 for validation\n",
    "train_fold = -1 * np.ones(len(y_train), dtype=int)\n",
    "val_fold = 0 * np.ones(len(y_val), dtype=int)\n",
    "test_fold = np.concatenate([train_fold, val_fold])\n",
    "predef_split = PredefinedSplit(test_fold)\n",
    "\n",
    "# Define an expanded parameter grid for QDA\n",
    "uniform_priors = np.ones(n_classes) / n_classes  \n",
    "param_dist = {\n",
    "    \"reg_param\": np.linspace(0.0, 1.0, 21),   \n",
    "    \"priors\":    [None, uniform_priors],\n",
    "}\n",
    "\n",
    "qda_base = QuadraticDiscriminantAnalysis(store_covariance=False)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=qda_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,           \n",
    "    cv=predef_split,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,            \n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    refit=False,\n",
    ")\n",
    "\n",
    "start_rs_time = time.time()\n",
    "random_search.fit(X_combined, y_combined)\n",
    "end_rs_time   = time.time()\n",
    "grid_search_time = end_rs_time - start_rs_time\n",
    "\n",
    "print(f\"[INFO] RandomizedSearchCV completed in {grid_search_time:.2f} s.\")\n",
    "print(\"[INFO] Best hyper-parameters:\", random_search.best_params_)\n",
    "print(f\"[INFO] Best validation accuracy: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# --------------Train Final Model with Best Hyperparameters--------------------------\n",
    "print(\"[INFO] Training final QDA with best hyperparameters on X_train only...\")\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "best_mlc = QuadraticDiscriminantAnalysis(**best_params, store_covariance=False)\n",
    "\n",
    "start_train_time = time.time()\n",
    "best_mlc.fit(X_train, y_train)  # Removed sample_weight\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "\n",
    "print(f\"[INFO] Done training final MLC model in {training_time:.2f} seconds.\")\n",
    "\n",
    "# ---------------------Validation Evaluation--------------------------------------\n",
    "print(\"\\n[INFO] Validation Set Evaluation...\")\n",
    "start_val_eval = time.time()\n",
    "y_val_pred = best_mlc.predict(X_val)\n",
    "end_val_eval = time.time()\n",
    "\n",
    "validation_time = end_val_eval - start_val_eval\n",
    "val_cm = confusion_matrix(y_val, y_val_pred)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "val_kappa = cohen_kappa_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\" - Finished in {validation_time:.2f} seconds.\")\n",
    "print(\" - Confusion Matrix (Validation):\\n\", val_cm)\n",
    "print(f\" - Accuracy: {val_acc:.4f}\")\n",
    "print(f\" - Cohen's Kappa: {val_kappa:.4f}\")\n",
    "\n",
    "# --------------------------Test Evaluation---------------------------------------\n",
    "print(\"\\n[INFO] Test Set Evaluation...\")\n",
    "start_test_eval = time.time()\n",
    "y_test_pred = best_mlc.predict(X_test)\n",
    "end_test_eval = time.time()\n",
    "\n",
    "test_time = end_test_eval - start_test_eval\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_kappa = cohen_kappa_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\" - Finished in {test_time:.2f} seconds.\")\n",
    "print(\" - Confusion Matrix (Test):\\n\", test_cm)\n",
    "print(f\" - Accuracy: {test_acc:.4f}\")\n",
    "print(f\" - Cohen's Kappa: {test_kappa:.4f}\")\n",
    "\n",
    "# ---------Save Model, Confusion Matrix, and Evaluation Metrics---------------------\n",
    "print(\"\\n[INFO] Saving model and evaluation outputs...\")\n",
    "model_path = os.path.join(OUTPUT_DIR, \"best_mlc_model.pth\")\n",
    "joblib.dump(best_mlc, model_path)\n",
    "print(f\" - Best MLC model saved to: {model_path}\")\n",
    "\n",
    "conf_matrix_txt_path = os.path.join(OUTPUT_DIR, \"test_confusion_matrix.txt\")\n",
    "np.savetxt(conf_matrix_txt_path, test_cm, delimiter=',', fmt='%d')\n",
    "print(f\" - Confusion matrix saved as text file to {conf_matrix_txt_path}\")\n",
    "\n",
    "overall_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "overall_kappa = cohen_kappa_score(y_test, y_test_pred)\n",
    "overall_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "per_class_f1 = f1_score(y_test, y_test_pred, average=None)\n",
    "\n",
    "dice_scores = []\n",
    "iou_scores = []\n",
    "for i in range(n_classes):\n",
    "    TP = test_cm[i, i]\n",
    "    FP = np.sum(test_cm[:, i]) - TP\n",
    "    FN = np.sum(test_cm[i, :]) - TP\n",
    "    denominator_dice = (2 * TP + FP + FN)\n",
    "    denominator_iou = (TP + FP + FN)\n",
    "    \n",
    "    dice = (2 * TP / denominator_dice) if denominator_dice > 0 else 0\n",
    "    iou = (TP / denominator_iou) if denominator_iou > 0 else 0\n",
    "    \n",
    "    dice_scores.append(dice)\n",
    "    iou_scores.append(iou)\n",
    "\n",
    "jaccard_indices = iou_scores.copy()\n",
    "\n",
    "metrics_str = \"Overall Metrics:\\n\"\n",
    "metrics_str += f\"Accuracy: {overall_accuracy:.4f}\\n\"\n",
    "metrics_str += f\"Cohen's Kappa: {overall_kappa:.4f}\\n\"\n",
    "metrics_str += f\"Weighted F1-Score: {overall_f1:.4f}\\n\\n\"\n",
    "metrics_str += \"Per-Class Metrics:\\n\"\n",
    "metrics_str += \"Class\\tF1-Score\\tDice Coefficient\\tIoU (Jaccard Index)\\n\"\n",
    "for i in range(n_classes):\n",
    "    metrics_str += f\"{i}\\t{per_class_f1[i]:.4f}\\t\\t{dice_scores[i]:.4f}\\t\\t\\t{iou_scores[i]:.4f}\\n\"\n",
    "\n",
    "metrics_str += \"\\nComputational Time Metrics (in seconds):\\n\"\n",
    "metrics_str += f\"RandomizedSearchCV Time: {grid_search_time:.2f}\\n\"\n",
    "metrics_str += f\"Training Time: {training_time:.2f}\\n\"\n",
    "metrics_str += f\"Validation Evaluation Time: {validation_time:.2f}\\n\"\n",
    "metrics_str += f\"Test Evaluation Time: {test_time:.2f}\\n\"\n",
    "total_time = grid_search_time + training_time + validation_time + test_time\n",
    "metrics_str += f\"Total Computational Time: {total_time:.2f}\\n\"\n",
    "\n",
    "metrics_file_path = os.path.join(OUTPUT_DIR, \"evaluation_metrics.txt\")\n",
    "with open(metrics_file_path, 'w') as f:\n",
    "    f.write(metrics_str)\n",
    "print(f\" - Evaluation metrics saved to: {metrics_file_path}\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\n[INFO] Done! Enhanced QDA-based MLC training & evaluation completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
