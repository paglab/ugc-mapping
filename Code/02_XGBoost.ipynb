{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb4857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected filesâ†’\n",
      "\n",
      "Train images (14):\n",
      "   RGB_2021-07-21 garden Karlsfeld.tif\n",
      "   RGB_2021-07-21 garden Sonnengarten Solln.tif\n",
      "   RGB_2021-07-21 garden Stadtacker.tif\n",
      "   RGB_2021-07-22 gardens_munich essbare_Stadt.tif\n",
      "   RGB_2021-08-17 garden Freiluftgarten Freiham.tif\n",
      "   RGB_2021-08-17 garden Sonnengarten Solln.tif\n",
      "   RGB_2021-08-17 garden Stadtacker.tif\n",
      "   RGB_2022-05-10 garden Karlsfeld.tif\n",
      "   RGB_2022-07-19 garden Freiluftgarten Freiham.tif\n",
      "   RGB_2022-07-19 garden Sonnengarten Solln.tif\n",
      "   RGB_2022-07-19 gardens_munich essbare_Stadt.tif\n",
      "   RGB_2022-10-20 garden Freiluftgarten Freiham.tif\n",
      "   RGB_2022-10-20 garden Stadtacker.tif\n",
      "   RGB_2022-10-20 gardens_munich essbare_Stadt.tif\n",
      "Train masks   (14):\n",
      "   RGB_2021-07-21 garden Karlsfeld_Labelled.tif\n",
      "   RGB_2021-07-21 garden Sonnengarten Solln_Labelled.tif\n",
      "   RGB_2021-07-21 garden Stadtacker_Labelled.tif\n",
      "   RGB_2021-07-22 gardens_munich essbare_Stadt_Labelled.tif\n",
      "   RGB_2021-08-17 garden Freiluftgarten Freiham_Labelled.tif\n",
      "   RGB_2021-08-17 garden Sonnengarten Solln_Labelled.tif\n",
      "   RGB_2021-08-17 garden Stadtacker_Labelled.tif\n",
      "   RGB_2022-05-10 garden Karlsfeld_Labelled.tif\n",
      "   RGB_2022-07-19 garden Freiluftgarten Freiham_Labelled.tif\n",
      "   RGB_2022-07-19 garden Sonnengarten Solln_Labelled.tif\n",
      "   RGB_2022-07-19 gardens_munich essbare_Stadt_Labelled.tif\n",
      "   RGB_2022-10-20 garden Freiluftgarten Freiham_Labelled.tif\n",
      "   RGB_2022-10-20 garden Stadtacker_Labelled.tif\n",
      "   RGB_2022-10-20 gardens_munich essbare_Stadt_Labelled.tif\n",
      "\n",
      "Val images (5):\n",
      "   RGB_2021-08-17 gardens_munich essbare_Stadt.tif\n",
      "   RGB_2022-05-10 garden Freiluftgarten Freiham.tif\n",
      "   RGB_2022-05-10 garden Sonnengarten Solln.tif\n",
      "   RGB_2022-07-19 garden Stadtacker.tif\n",
      "   RGB_2022-10-21 garden Karlsfeld.tif\n",
      "Val masks   (5):\n",
      "   RGB_2021-08-17 gardens_munich essbare_Stadt_Labelled.tif\n",
      "   RGB_2022-05-10 garden Freiluftgarten Freiham_Labelled.tif\n",
      "   RGB_2022-05-10 garden Sonnengarten Solln_Labelled.tif\n",
      "   RGB_2022-07-19 garden Stadtacker_Labelled.tif\n",
      "   RGB_2022-10-21 garden Karlsfeld_Labelled.tif\n",
      "\n",
      "Test images (5):\n",
      "   RGB_2021-07-21 garden Freiluftgarten Freiham.tif\n",
      "   RGB_2021-08-17 garden Karlsfeld.tif\n",
      "   RGB_2022-05-10 garden Stadtacker.tif\n",
      "   RGB_2022-05-10 gardens_munich essbare_Stadt.tif\n",
      "   RGB_2022-10-20 garden Sonnengarten Solln.tif\n",
      "Test masks   (5):\n",
      "   RGB_2021-07-21 garden Freiluftgarten Freiham_Labelled.tif\n",
      "   RGB_2021-08-17 garden Karlsfeld_Labelled.tif\n",
      "   RGB_2022-05-10 garden Stadtacker_Labelled.tif\n",
      "   RGB_2022-05-10 gardens_munich essbare_Stadt_Labelled.tif\n",
      "   RGB_2022-10-20 garden Sonnengarten Solln_Labelled.tif\n",
      "[INFO] Building Datasets...\n",
      "\n",
      "[Dataset Initialization] kn=5, max_patches_per_image=None\n",
      "[INFO] Finished extracting patches for 1/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2021-07-21 garden Karlsfeld.tif\n",
      "[INFO] Finished extracting patches for 2/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2021-07-21 garden Sonnengarten Solln.tif\n",
      "[INFO] Finished extracting patches for 3/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2021-07-21 garden Stadtacker.tif\n",
      "[INFO] Finished extracting patches for 4/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2021-07-22 gardens_munich essbare_Stadt.tif\n",
      "[INFO] Finished extracting patches for 5/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2021-08-17 garden Freiluftgarten Freiham.tif\n",
      "[INFO] Finished extracting patches for 6/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2021-08-17 garden Sonnengarten Solln.tif\n",
      "[INFO] Finished extracting patches for 7/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2021-08-17 garden Stadtacker.tif\n",
      "[INFO] Finished extracting patches for 8/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2022-05-10 garden Karlsfeld.tif\n",
      "[INFO] Finished extracting patches for 9/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2022-07-19 garden Freiluftgarten Freiham.tif\n",
      "[INFO] Finished extracting patches for 10/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2022-07-19 garden Sonnengarten Solln.tif\n",
      "[INFO] Finished extracting patches for 11/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2022-07-19 gardens_munich essbare_Stadt.tif\n",
      "[INFO] Finished extracting patches for 12/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2022-10-20 garden Freiluftgarten Freiham.tif\n",
      "[INFO] Finished extracting patches for 13/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2022-10-20 garden Stadtacker.tif\n",
      "[INFO] Finished extracting patches for 14/14: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\train\\RGB_2022-10-20 gardens_munich essbare_Stadt.tif\n",
      "[INFO] Dataset created with total patches: 3063488\n",
      "Memory Usage: 16596.89 MB\n",
      "\n",
      "[Dataset Initialization] kn=5, max_patches_per_image=None\n",
      "[INFO] Finished extracting patches for 1/5: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\val\\RGB_2021-08-17 gardens_munich essbare_Stadt.tif\n",
      "[INFO] Finished extracting patches for 2/5: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\val\\RGB_2022-05-10 garden Freiluftgarten Freiham.tif\n",
      "[INFO] Finished extracting patches for 3/5: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\val\\RGB_2022-05-10 garden Sonnengarten Solln.tif\n",
      "[INFO] Finished extracting patches for 4/5: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\val\\RGB_2022-07-19 garden Stadtacker.tif\n",
      "[INFO] Finished extracting patches for 5/5: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\val\\RGB_2022-10-21 garden Karlsfeld.tif\n",
      "[INFO] Dataset created with total patches: 1452517\n",
      "Memory Usage: 20354.89 MB\n",
      "\n",
      "[Dataset Initialization] kn=5, max_patches_per_image=None\n",
      "[INFO] Finished extracting patches for 1/5: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\test\\RGB_2021-07-21 garden Freiluftgarten Freiham.tif\n",
      "[INFO] Finished extracting patches for 2/5: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\test\\RGB_2021-08-17 garden Karlsfeld.tif\n",
      "[INFO] Finished extracting patches for 3/5: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\test\\RGB_2022-05-10 garden Stadtacker.tif\n",
      "[INFO] Finished extracting patches for 4/5: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\test\\RGB_2022-05-10 gardens_munich essbare_Stadt.tif\n",
      "[INFO] Finished extracting patches for 5/5: D:\\Yasamin\\02_Workshop\\Models_V2\\02_dataset_split2\\test\\RGB_2022-10-20 garden Sonnengarten Solln.tif\n",
      "[INFO] Dataset created with total patches: 1072618\n",
      "Memory Usage: 23057.60 MB\n",
      "\n",
      "[INFO] Finished building all datasets in 125.70 seconds.\n",
      "[INFO] Converting training patches into (X_train, y_train)...\n",
      " - Done. Shapes: X_train=(3063488, 363), y_train=(3063488,)\n",
      "[INFO] Converting validation patches into (X_val, y_val)...\n",
      " - Done. Shapes: X_val=(1452517, 363), y_val=(1452517,)\n",
      "[INFO] Converting test patches into (X_test, y_test)...\n",
      " - Done. Shapes: X_test=(1072618, 363), y_test=(1072618,)\n",
      "\n",
      "[INFO] Starting Hyperparameter Search using RandomizedSearchCV...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'estimator_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 281\u001b[0m\n\u001b[0;32m    269\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m    270\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mxgb_base,\n\u001b[0;32m    271\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_dist,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    277\u001b[0m     refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# We'll refit manually after selecting best hyperparameters\u001b[39;00m\n\u001b[0;32m    278\u001b[0m )\n\u001b[0;32m    280\u001b[0m start_rs_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 281\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_combined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m end_rs_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    283\u001b[0m rs_search_time \u001b[38;5;241m=\u001b[39m end_rs_time \u001b[38;5;241m-\u001b[39m start_rs_time\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:933\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    929\u001b[0m params \u001b[38;5;241m=\u001b[39m _check_method_params(X, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m    931\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_routed_params_for_fit(params)\n\u001b[1;32m--> 933\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39m\u001b[43mis_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    934\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)\n\u001b[0;32m    936\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1237\u001b[0m, in \u001b[0;36mis_classifier\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m   1230\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1231\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassing a class to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mprint\u001b[39m(inspect\u001b[38;5;241m.\u001b[39mstack()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in 1.8. Use an instance of the class instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1233\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1234\u001b[0m     )\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_estimator_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator_type\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'estimator_type'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# ---------------------------------------------------------------------\n",
    "#      XGBoost Script for Ground-Cover Classification\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn tools\n",
    "from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib  # for model saving/loading\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# XGBoost (sklearn API)\n",
    "from xgboost import XGBClassifier, XGBModel\n",
    "\n",
    "# --------------------------- portable paths -------------------------\n",
    "REPO_ROOT   = Path(__file__).resolve().parent\n",
    "DATASET_DIR = Path(os.getenv(\"DATASET\", REPO_ROOT / \"Dataset\"))\n",
    "OUTPUT_DIR  = Path(os.getenv(\"XGB_OUTPUT\",  REPO_ROOT / \"Output\"))\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_PATH = DATASET_DIR / \"train\"\n",
    "VAL_PATH   = DATASET_DIR / \"val\"\n",
    "TEST_PATH  = DATASET_DIR / \"test\"\n",
    "# ------------------------Discover RGB images & their masks--------------------------------\n",
    "def list_rgb_and_masks(folder: str):\n",
    "    rgb = sorted([\n",
    "        os.path.join(folder, f)\n",
    "        for f in os.listdir(folder)\n",
    "        if f.lower().endswith(\".tif\") and not f.lower().endswith(\"_labelled.tif\")\n",
    "    ])\n",
    "    masks = sorted([\n",
    "        os.path.join(folder, f)\n",
    "        for f in os.listdir(folder)\n",
    "        if f.lower().endswith(\"_labelled.tif\")\n",
    "    ])\n",
    "    return rgb, masks\n",
    "\n",
    "train_image_files, train_mask_files = list_rgb_and_masks(TRAIN_PATH)\n",
    "val_image_files,   val_mask_files   = list_rgb_and_masks(VAL_PATH)\n",
    "test_image_files,  test_mask_files  = list_rgb_and_masks(TEST_PATH)\n",
    "\n",
    "# Optional: print a quick overview\n",
    "print(\"Collected filesâ†’\")\n",
    "for split, imgs, msks in [(\"Train\", train_image_files, train_mask_files),\n",
    "                          (\"Val\",   val_image_files,   val_mask_files),\n",
    "                          (\"Test\",  test_image_files,  test_mask_files)]:\n",
    "    print(f\"\\n{split} images ({len(imgs)}):\")\n",
    "    for p in imgs:\n",
    "        print(\"  \", os.path.basename(p))\n",
    "    print(f\"{split} masks   ({len(msks)}):\")\n",
    "    for p in msks:\n",
    "        print(\"  \", os.path.basename(p))\n",
    "\n",
    "n_classes = 8           # background = 0 ignored during patch extraction\n",
    "MAX_PATCHES_PER_IMAGE = 10000\n",
    "# -----------------------Utility Functions---------------------------------------\n",
    "def print_memory_usage():\n",
    "    mem = psutil.Process().memory_info().rss / 1024 ** 2\n",
    "    print(f\"Memory Usage: {mem:.2f} MB\")\n",
    "\n",
    "def get_label_file(image_file: str) -> str:\n",
    "    \"\"\"Return mask path matching *image_file* (expects suffix `_Labelled.tif`).\"\"\"\n",
    "    base = os.path.splitext(image_file)[0]\n",
    "    label_file = f\"{base}_Labelled.tif\"\n",
    "    if os.path.exists(label_file):\n",
    "        return label_file\n",
    "    raise FileNotFoundError(f\"No label file found for {image_file}\")\n",
    "\n",
    "\n",
    "def load_and_normalize_data(image_file):\n",
    "    try:\n",
    "        data = tifffile.imread(image_file)\n",
    "        if data.ndim == 2:\n",
    "            data = np.stack([data] * 3, axis=-1)\n",
    "        elif data.shape[-1] != 3:\n",
    "            raise ValueError(f\"Unexpected image shape: {data.shape}, expected 3 channels\")\n",
    "        \n",
    "        label_file = get_label_file(image_file)\n",
    "        labels = tifffile.imread(label_file)\n",
    "        \n",
    "        data_min, data_max = np.min(data), np.max(data)\n",
    "        data = (data - data_min) / (data_max - data_min + 1e-8)\n",
    "        data = data.astype(np.float32)\n",
    "        labels = labels.astype(np.int32)\n",
    "        return data, labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {image_file}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# -----------------------Patch Extraction---------------------------------------\n",
    "def extract_patches(image, label, kn, max_patches_per_image=None):\n",
    "    patch_size = 2 * kn + 1\n",
    "    padded_img = np.pad(image, ((kn, kn), (kn, kn), (0, 0)), mode='reflect')\n",
    "    padded_lbl = np.pad(label, ((kn, kn), (kn, kn)), mode='constant', constant_values=0)\n",
    "\n",
    "    # 1) gather all valid coords\n",
    "    coords = [(i,j) for i in range(label.shape[0])\n",
    "                     for j in range(label.shape[1])\n",
    "                     if label[i,j] != 0]\n",
    "\n",
    "    # 2) if you want to limit to K patches, sample from coords\n",
    "    if max_patches_per_image and len(coords) > max_patches_per_image:\n",
    "        sampled_idxs = np.random.choice(len(coords),\n",
    "                                        max_patches_per_image,\n",
    "                                        replace=False)\n",
    "        coords = [coords[i] for i in sampled_idxs]\n",
    "\n",
    "    # 3) now build your patches & labels\n",
    "    patches, labels = [], []\n",
    "    for i,j in coords:\n",
    "        patch = padded_img[i:i+patch_size, j:j+patch_size, :]\n",
    "        patches.append(patch)\n",
    "        labels.append(label[i,j] - 1)   # zero-based\n",
    "    return patches, labels\n",
    "\n",
    "# ---------------------Custom \"Dataset\" to Gather Patches----------------------------------\n",
    "class PatchDataset:\n",
    "    def __init__(self, image_files, kn=5, max_patches_per_image=None):\n",
    "        self.image_files = image_files\n",
    "        self.kn = kn\n",
    "        self.max_patches_per_image = max_patches_per_image\n",
    "\n",
    "        self.all_patches = []\n",
    "        self.all_labels = []\n",
    "        total_patches = 0\n",
    "\n",
    "        print(f\"\\n[Dataset Initialization] kn={kn}, max_patches_per_image={max_patches_per_image}\")\n",
    "        for idx, img_path in enumerate(self.image_files, start=1):\n",
    "            img_array, lbl_array = load_and_normalize_data(img_path)\n",
    "            if img_array is None or lbl_array is None:\n",
    "                print(f\"[WARNING] Skipping file due to error: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            patches, labels = extract_patches(img_array, lbl_array, self.kn, self.max_patches_per_image)\n",
    "            self.all_patches.extend(patches)\n",
    "            self.all_labels.extend(labels)\n",
    "            total_patches += len(patches)\n",
    "\n",
    "            print(f\"[INFO] Finished extracting patches for {idx}/{len(self.image_files)}: {img_path}\")\n",
    "        \n",
    "        print(f\"[INFO] Dataset created with total patches: {total_patches}\")\n",
    "        print_memory_usage()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.all_patches[idx], self.all_labels[idx]\n",
    "\n",
    "# -----------------------Build Datasets (Train, Val, Test)--------------------------------\n",
    "print(\"[INFO] Building Datasets...\")\n",
    "kn = 5  # half patch size\n",
    "start_build_time = time.time()\n",
    "\n",
    "train_dataset = PatchDataset(train_image_files, kn, MAX_PATCHES_PER_IMAGE)\n",
    "val_dataset   = PatchDataset(val_image_files,   kn, MAX_PATCHES_PER_IMAGE)\n",
    "test_dataset  = PatchDataset(test_image_files,  kn, MAX_PATCHES_PER_IMAGE)\n",
    "\n",
    "end_build_time = time.time()\n",
    "print(f\"\\n[INFO] Finished building all datasets in {end_build_time - start_build_time:.2f} seconds.\")\n",
    "\n",
    "# Convert each dataset into X (features) and y (labels) for scikit-learn\n",
    "def dataset_to_Xy(patch_dataset):\n",
    "    patches = patch_dataset.all_patches\n",
    "    labels  = patch_dataset.all_labels\n",
    "    # Flatten each patch: shape => ( (2*kn+1)^2 * 3 )\n",
    "    X = np.array([p.reshape(-1) for p in patches], dtype=np.float32)\n",
    "    y = np.array(labels, dtype=np.int64)\n",
    "    return X, y\n",
    "\n",
    "print(\"[INFO] Converting training patches into (X_train, y_train)...\")\n",
    "X_train, y_train = dataset_to_Xy(train_dataset)\n",
    "print(f\" - Done. Shapes: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "\n",
    "print(\"[INFO] Converting validation patches into (X_val, y_val)...\")\n",
    "X_val, y_val = dataset_to_Xy(val_dataset)\n",
    "print(f\" - Done. Shapes: X_val={X_val.shape}, y_val={y_val.shape}\")\n",
    "\n",
    "print(\"[INFO] Converting test patches into (X_test, y_test)...\")\n",
    "X_test, y_test = dataset_to_Xy(test_dataset)\n",
    "print(f\" - Done. Shapes: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "# -----------XGBoost: Hyperparameter Tuning using RandomizedSearchCV------------------\n",
    "print(\"\\n[INFO] Starting Hyperparameter Search using RandomizedSearchCV...\")\n",
    "# Combine training and validation data for hyperparameter search\n",
    "X_combined = np.concatenate([X_train, X_val], axis=0)\n",
    "y_combined = np.concatenate([y_train, y_val], axis=0)\n",
    "\n",
    "# Create a predefined split: -1 for training samples, 0 for validation samples\n",
    "train_fold = -1 * np.ones(len(y_train), dtype=int)\n",
    "val_fold = 0 * np.ones(len(y_val), dtype=int)\n",
    "test_fold = np.concatenate([train_fold, val_fold])\n",
    "predef_split = PredefinedSplit(test_fold)\n",
    "\n",
    "# Define the hyperparameter distributions for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 6, 9, 12, 15],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV with 50 iterations\n",
    "xgb_base = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=n_classes,\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='accuracy',\n",
    "    cv=predef_split,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    refit=False  \n",
    ")\n",
    "\n",
    "start_rs_time = time.time()\n",
    "random_search.fit(X_combined, y_combined)\n",
    "end_rs_time = time.time()\n",
    "rs_search_time = end_rs_time - start_rs_time\n",
    "\n",
    "print(f\"[INFO] RandomizedSearchCV completed in {rs_search_time:.2f} seconds.\")\n",
    "print(f\"[INFO] Best Hyperparameters found: {random_search.best_params_}\")\n",
    "print(f\"[INFO] Best validation accuracy: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# --------------Train Final Model with Best Hyperparameters--------------------------\n",
    "print(\"[INFO] Training final XGBoost with best hyperparameters on full training set...\")\n",
    "best_params = random_search.best_params_\n",
    "best_xgb = XGBClassifier(\n",
    "    **best_params,\n",
    "    objective='multi:softmax',\n",
    "    num_class=n_classes,\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Optional: Compute sample weights for class imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "sample_weights = np.array([class_weights[label] for label in y_train])\n",
    "\n",
    "start_train_time = time.time()\n",
    "best_xgb.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "\n",
    "print(f\"[INFO] Done training final model in {training_time:.2f} seconds.\")\n",
    "\n",
    "# ---------------------Validation Evaluation--------------------------------------\n",
    "print(\"\\n[INFO] Validation Set Evaluation...\")\n",
    "start_val_eval = time.time()\n",
    "y_val_pred = best_xgb.predict(X_val)\n",
    "end_val_eval = time.time()\n",
    "\n",
    "validation_time = end_val_eval - start_val_eval\n",
    "val_cm = confusion_matrix(y_val, y_val_pred)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "val_kappa = cohen_kappa_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\" - Finished in {validation_time:.2f} seconds.\")\n",
    "print(\" - Confusion Matrix (Validation):\\n\", val_cm)\n",
    "print(f\" - Accuracy: {val_acc:.4f}\")\n",
    "print(f\" - Cohen's Kappa: {val_kappa:.4f}\")\n",
    "\n",
    "# --------------------------Test Evaluation---------------------------------------\n",
    "print(\"\\n[INFO] Test Set Evaluation...\")\n",
    "start_test_eval = time.time()\n",
    "y_test_pred = best_xgb.predict(X_test)\n",
    "end_test_eval = time.time()\n",
    "\n",
    "test_time = end_test_eval - start_test_eval\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_kappa = cohen_kappa_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\" - Finished in {test_time:.2f} seconds.\")\n",
    "print(\" - Confusion Matrix (Test):\\n\", test_cm)\n",
    "print(f\" - Accuracy: {test_acc:.4f}\")\n",
    "print(f\" - Cohen's Kappa: {test_kappa:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 11. Save Model, Confusion Matrix, and Evaluation Metrics\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n[INFO] Saving model and evaluation outputs...\")\n",
    "# Save the best model using joblib\n",
    "model_path = os.path.join(OUTPUT_DIR, \"best_xgb_model.pth\")\n",
    "joblib.dump(best_xgb, model_path)\n",
    "print(f\" - Best XGBoost model saved to: {model_path}\")\n",
    "\n",
    "# Save confusion matrix as a text file \n",
    "conf_matrix_txt_path = os.path.join(OUTPUT_DIR, \"test_confusion_matrix.txt\")\n",
    "np.savetxt(conf_matrix_txt_path, test_cm, delimiter=',', fmt='%d')\n",
    "print(f\" - Confusion matrix saved as text file to {conf_matrix_txt_path}\")\n",
    "\n",
    "# Compute overall evaluation metrics\n",
    "overall_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "overall_kappa = cohen_kappa_score(y_test, y_test_pred)\n",
    "overall_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "# Compute per-class F1-scores\n",
    "per_class_f1 = f1_score(y_test, y_test_pred, average=None)\n",
    "\n",
    "# Initialize lists to store per-class Dice and IoU metrics\n",
    "dice_scores = []\n",
    "iou_scores = []\n",
    "\n",
    "# Calculate per-class Dice and IoU metrics using the confusion matrix\n",
    "for i in range(n_classes):\n",
    "    TP = test_cm[i, i]\n",
    "    FP = np.sum(test_cm[:, i]) - TP\n",
    "    FN = np.sum(test_cm[i, :]) - TP\n",
    "    denominator_dice = (2 * TP + FP + FN)\n",
    "    denominator_iou = (TP + FP + FN)\n",
    "    \n",
    "    dice = (2 * TP / denominator_dice) if denominator_dice > 0 else 0\n",
    "    iou = (TP / denominator_iou) if denominator_iou > 0 else 0\n",
    "    \n",
    "    dice_scores.append(dice)\n",
    "    iou_scores.append(iou)\n",
    "\n",
    "# Since IoU and Jaccard Index are equivalent, we assign the same values.\n",
    "jaccard_indices = iou_scores.copy()\n",
    "\n",
    "# Prepare a string to save the metrics, including computational times\n",
    "metrics_str = \"Overall Metrics:\\n\"\n",
    "metrics_str += f\"Accuracy: {overall_accuracy:.4f}\\n\"\n",
    "metrics_str += f\"Cohen's Kappa: {overall_kappa:.4f}\\n\"\n",
    "metrics_str += f\"Weighted F1-Score: {overall_f1:.4f}\\n\\n\"\n",
    "metrics_str += \"Per-Class Metrics:\\n\"\n",
    "metrics_str += \"Class\\tF1-Score\\tDice Coefficient\\tIoU (Jaccard Index)\\n\"\n",
    "for i in range(n_classes):\n",
    "    metrics_str += f\"{i}\\t{per_class_f1[i]:.4f}\\t\\t{dice_scores[i]:.4f}\\t\\t\\t{iou_scores[i]:.4f}\\n\"\n",
    "\n",
    "# Append computational time metrics\n",
    "metrics_str += \"\\nComputational Time Metrics (in seconds):\\n\"\n",
    "metrics_str += f\"RandomizedSearchCV Time: {rs_search_time:.2f}\\n\"\n",
    "metrics_str += f\"Training Time: {training_time:.2f}\\n\"\n",
    "metrics_str += f\"Validation Evaluation Time: {validation_time:.2f}\\n\"\n",
    "metrics_str += f\"Test Evaluation Time: {test_time:.2f}\\n\"\n",
    "total_time = rs_search_time + training_time + validation_time + test_time\n",
    "metrics_str += f\"Total Computational Time: {total_time:.2f}\\n\"\n",
    "\n",
    "# Define the path for saving the metrics file\n",
    "metrics_file_path = os.path.join(OUTPUT_DIR, \"evaluation_metrics.txt\")\n",
    "\n",
    "# Save the metrics to the text file\n",
    "with open(metrics_file_path, 'w') as f:\n",
    "    f.write(metrics_str)\n",
    "\n",
    "print(f\" - Evaluation metrics saved to: {metrics_file_path}\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\n[INFO] Done! Improved XGBoost training & evaluation completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
